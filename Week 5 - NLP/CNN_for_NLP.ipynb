{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxEcEPquOVU3",
        "outputId": "b74e5650-a6e1-473f-98f1-c85a6e291d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6VaLcntff-A_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "reviews_file = '/content/drive/MyDrive/reviews.txt'\n",
        "labels_file = '/content/drive/MyDrive/labels.txt'\n",
        "\n",
        "# Read reviews.txt\n",
        "with open(reviews_file, 'r') as fileR:\n",
        "    reviews = fileR.read()\n",
        "\n",
        "# Read labels.txt\n",
        "with open(labels_file, 'r') as fileL:\n",
        "    labels = fileL.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYvV2AmRRUJR",
        "outputId": "d25823a8-b48b-4ed2-b44e-4590970dcbcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
            "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n",
            "\n",
            "positive\n",
            "n\n"
          ]
        }
      ],
      "source": [
        "print(reviews[:1000])\n",
        "print()\n",
        "print(labels[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4qmlh4tuR5Fg"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "\n",
        "reviews = reviews.lower()\n",
        "#removing punctuation\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
        "\n",
        "#making list of all reviews\n",
        "reviews_split = all_text.split('\\n')\n",
        "\n",
        "#getting all test\n",
        "all_text = ' '.join(reviews_split)\n",
        "\n",
        "all_words = all_text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T9NZsNWUUvPB",
        "outputId": "5bbb2667-803f-4eee-d913-00cd2ebf564d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "labels_split = labels.split('\\n')\n",
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])\n",
        "encoded_labels[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om9ebmRgWbhf",
        "outputId": "61c2db45-ab00-41c1-e4d0-a6e02cd44c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero lenght : 1\n",
            "Max lenght : 2514\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#checking any review with zero length\n",
        "review_lens = Counter([len(x.split()) for x in reviews_split])\n",
        "print(\"Zero lenght : {}\".format(review_lens[0]))\n",
        "print(\"Max lenght : {}\".format(max(review_lens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1EwAgHadYgVJ"
      },
      "outputs": [],
      "source": [
        "non_zero_idx = [ii for ii, review in enumerate(reviews_split) if len(review.split()) != 0]\n",
        "\n",
        "reviews_split = [reviews_split[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "#removing empty reviews alongwith their corresponding label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7qeCGI2YZyaZ",
        "outputId": "657d0684-145f-4a63-a2a8-dbadeb2c469b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-25 16:02:09--  https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz?download=\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/eyaler/word2vec-slim/master/GoogleNews-vectors-negative300-SLIM.bin.gz?download=true [following]\n",
            "--2024-06-25 16:02:09--  https://media.githubusercontent.com/media/eyaler/word2vec-slim/master/GoogleNews-vectors-negative300-SLIM.bin.gz?download=true\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 276467217 (264M) [application/octet-stream]\n",
            "Saving to: ‘GoogleNews-vectors-negative300-SLIM.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>] 263.66M   246MB/s    in 1.1s    \n",
            "\n",
            "2024-06-25 16:02:16 (246 MB/s) - ‘GoogleNews-vectors-negative300-SLIM.bin.gz’ saved [276467217/276467217]\n",
            "\n",
            "[ 9.30642411e-02  3.56607750e-04  1.60646606e-02  4.56457920e-02\n",
            " -2.47063395e-02  6.95765987e-02  1.16108716e-01 -9.92685184e-02\n",
            " -1.50259968e-03  9.52800512e-02 -9.39505622e-02  8.03233031e-03\n",
            "  1.94991734e-02 -9.12915841e-02 -7.13492483e-02  8.55304673e-02\n",
            " -1.69509854e-02  1.07467035e-02 -1.19654022e-01 -6.07133359e-02\n",
            "  1.01484336e-01  9.30642411e-02 -2.64789909e-02 -1.41258221e-02\n",
            "  2.23312629e-04  1.07245453e-01  5.29579818e-02  1.92775931e-02\n",
            " -6.07133359e-02  8.37578177e-02  2.28228960e-02 -2.72545274e-02\n",
            " -2.82516442e-02  3.45667191e-02  6.73607811e-02  2.76976898e-02\n",
            "  2.90271789e-02 -1.24085650e-01  6.73607811e-02  3.70041132e-02\n",
            "  7.13492483e-02 -1.19654024e-02 -5.02990037e-02  1.47351716e-02\n",
            " -3.14645767e-02 -1.49567528e-02 -6.11565001e-02  1.96099654e-02\n",
            " -6.47018030e-02 -1.13449737e-01  4.47594672e-02 -5.02990037e-02\n",
            " -3.16861570e-02 -1.11677088e-01  7.48945549e-02 -4.45378870e-02\n",
            " -7.79966936e-02 -5.45090549e-02  5.51737994e-02  6.82471097e-02\n",
            "  1.88344289e-02  9.80498269e-03 -3.72256972e-02 -2.26013158e-02\n",
            " -2.94703431e-02  3.85551862e-02 -3.16861570e-02  1.50675438e-02\n",
            "  6.51449636e-02 -5.98270111e-02  6.84132939e-03  4.91910987e-02\n",
            "  5.89406863e-02 -2.70329453e-02  2.99135055e-02 -3.25724818e-02\n",
            " -7.09060878e-02  4.80831899e-02  3.19908326e-03  5.53953797e-02\n",
            "  7.13492483e-02  1.35164727e-02  3.87767665e-02 -5.34011461e-02\n",
            " -4.03278358e-02  1.68401953e-02 -2.72822240e-03  1.89729175e-03\n",
            "  7.26787373e-02  7.14600412e-03  7.57808834e-02  7.69995805e-03\n",
            " -2.07178723e-02  4.47594672e-02  7.48945549e-02  3.52314636e-02\n",
            "  4.71968651e-02  6.95765987e-02 -3.15753673e-03 -4.27652337e-02\n",
            " -5.25148213e-02 -7.48945549e-02 -1.09461270e-01  3.94415110e-02\n",
            "  6.29291534e-02 -4.67537008e-02  8.86326097e-03 -3.12429946e-02\n",
            "  1.40704261e-02  3.41235548e-02 -6.11565001e-02 -3.03566679e-02\n",
            " -5.98270111e-02  3.70041132e-02  7.26787373e-02 -3.10214125e-02\n",
            "  3.34588103e-02  4.29868139e-02  4.12141643e-02  4.23220694e-02\n",
            "  7.47837638e-03 -1.23199329e-01 -2.08286624e-02 -4.98558432e-02\n",
            " -1.48182642e-03  8.19851607e-02 -1.52337295e-03  2.20196648e-03\n",
            "  1.71060935e-01  6.03809627e-03 -8.10988396e-02 -1.54220745e-01\n",
            " -1.13560529e-02  8.50873068e-02  1.12120248e-01 -5.14069125e-02\n",
            " -6.60312921e-02 -7.44513944e-02 -8.37578177e-02  7.62240440e-02\n",
            "  1.99423373e-01 -3.98846716e-03  2.14934070e-02  7.44513944e-02\n",
            "  7.44513944e-02 -1.63970329e-02 -3.21293212e-02  6.03809627e-03\n",
            " -8.90757740e-02 -5.65032894e-03  6.86902702e-02 -4.09925822e-03\n",
            " -8.06556717e-02 -3.74472775e-02 -3.14645767e-02  2.53710840e-02\n",
            " -5.84975220e-02  5.22932373e-02 -2.12718267e-02 -7.75535330e-02\n",
            "  1.90560110e-02 -1.73941497e-02  6.95765987e-02 -2.64789909e-02\n",
            "  2.82516447e-03  9.92685184e-02  5.00774235e-02 -6.95765987e-02\n",
            "  8.06046046e-06  5.84975220e-02 -8.73031169e-02  8.55304673e-02\n",
            " -9.74958688e-02 -2.39308048e-02 -2.59250384e-02  5.49522154e-02\n",
            "  3.65609527e-02 -6.86902702e-02  3.83336022e-02  2.55926661e-02\n",
            " -5.16284928e-02 -6.15996644e-02  3.74472775e-02 -2.61466186e-02\n",
            "  5.40658906e-02  1.15637854e-03 -4.95788641e-03  1.08574945e-02\n",
            " -1.16884252e-02 -7.92153925e-03 -9.83821973e-02  2.55926661e-02\n",
            "  7.35650659e-02 -6.47018030e-02 -7.79966936e-02  1.13560529e-02\n",
            "  3.07998322e-02 -3.50098796e-02  4.52026315e-02 -5.67248687e-02\n",
            "  6.09349180e-03  1.00154847e-01  2.13272218e-03  2.26013158e-02\n",
            " -7.97693506e-02  5.59493341e-03 -8.95189345e-02 -9.52800550e-03\n",
            "  1.77265219e-02 -3.62839736e-03  1.09018110e-01  1.49567528e-02\n",
            " -6.95765987e-02  3.21293212e-02 -1.02370664e-01 -5.06867748e-03\n",
            "  1.16108716e-01  2.90271789e-02 -9.26210731e-02  1.22313000e-01\n",
            " -4.23220694e-02  2.73653176e-02  2.22689435e-02 -1.39596360e-02\n",
            "  2.37092227e-02 -1.31287053e-02  4.52026315e-02  9.48368907e-02\n",
            "  1.62862409e-02 -4.25436534e-02 -4.05494198e-02 -5.51737994e-02\n",
            "  1.56214973e-02 -4.56457920e-02  2.08286624e-02 -1.63970329e-02\n",
            "  3.89983468e-02  2.00531278e-02  7.84398615e-02  6.23198040e-03\n",
            " -1.17438203e-02 -4.21004891e-02  7.17924163e-02  1.11344717e-02\n",
            "  3.96630913e-02 -6.29291534e-02 -8.33146498e-02  6.15996644e-02\n",
            " -6.11565001e-02 -2.04962902e-02  4.71968651e-02  1.41812172e-02\n",
            " -4.32083979e-02 -2.82516442e-02  9.07099340e-04 -1.97207555e-02\n",
            "  5.92730567e-03  6.64744526e-02 -5.62817045e-02  1.58430785e-02\n",
            " -5.09637482e-02  6.78039417e-02 -7.79966936e-02 -4.45378870e-02\n",
            "  9.61663797e-02  1.98315457e-02  1.91668011e-02  1.68401953e-02\n",
            " -7.31219053e-02  1.32394955e-02  3.33757163e-03 -1.10790757e-02\n",
            " -1.03589362e-02  2.46509444e-03 -2.92487610e-02 -1.49567528e-02\n",
            " -4.78616096e-02 -3.27940658e-02  1.29625192e-02 -3.81120220e-02\n",
            "  3.65609527e-02  2.83624344e-02 -3.78904417e-02 -5.07421680e-02\n",
            " -2.01196015e-01 -2.30444781e-02 -6.73607811e-02  8.33146498e-02\n",
            " -2.99135055e-02  4.25436534e-02 -3.45667191e-02 -2.64789909e-02\n",
            " -1.31176263e-01 -6.42586425e-02 -3.50098796e-02 -3.98846753e-02\n",
            "  4.40947227e-02  5.89406863e-02  5.71680330e-02  4.40947227e-02]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Step 1: Download the compressed .bin.gz file\n",
        "!wget -O GoogleNews-vectors-negative300-SLIM.bin.gz \"https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz?download=\"\n",
        "\n",
        "# Step 2: Decompress the downloaded file (if necessary)\n",
        "!gunzip GoogleNews-vectors-negative300-SLIM.bin.gz\n",
        "\n",
        "# Step 3: Load the Word2Vec model\n",
        "model_path = 'GoogleNews-vectors-negative300-SLIM.bin'\n",
        "embed_lookup = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
        "\n",
        "# Test the model\n",
        "print(embed_lookup['example'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_lookup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYwTJ4E_m7Vo",
        "outputId": "168438aa-2307-4ed0-ab0a-b800da88f856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.keyedvectors.KeyedVectors at 0x78c7d2a11510>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UY-begIEea98"
      },
      "outputs": [],
      "source": [
        "#embed_lookup is our dense embeddings (pre-trained word2vec model)\n",
        "pretrained_words = []\n",
        "for  index, word in enumerate(embed_lookup.index_to_key):\n",
        "  pretrained_words.append(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xpSzflekm2y7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HssS4ZtffBFt",
        "outputId": "8c0a8503-5c04-4a7b-d666-1da5f9a4e1bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Vocab: 299567\n",
            "\n",
            "Word in vocab: for\n",
            "\n",
            "Length of embedding: 300\n",
            "\n"
          ]
        }
      ],
      "source": [
        "row_idx = 1\n",
        "\n",
        "word= pretrained_words[row_idx]\n",
        "embedding = embed_lookup[word]\n",
        "\n",
        "print(\"Size of Vocab: {}\\n\".format(len(pretrained_words)))\n",
        "print('Word in vocab: {}\\n'.format(word))\n",
        "print('Length of embedding: {}\\n'.format(len(embedding)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we have all the words (Reviews_split)**  \n",
        "**vocabulary embedded (embed_lookup)**  \n",
        "**Now we will fetch the embedding of every word in reviews**"
      ],
      "metadata": {
        "id": "6uMQW9O9nzV9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tY6wHz9hgFuw"
      },
      "outputs": [],
      "source": [
        "def tokenize_all_reviews(embed_lookup, reviews_split):\n",
        "  #getting words list in east review\n",
        "  #reviews_split is list of reviews\n",
        "    reviews_words = [review.split() for review in reviews_split]\n",
        "\n",
        "    tokenized_reviews = []\n",
        "    for review in reviews_words:\n",
        "        ints = []\n",
        "        for word in review:\n",
        "            try:\n",
        "                idx = embed_lookup.key_to_index[word]\n",
        "            except:\n",
        "                idx = 0 #if not in looup, assign zero\n",
        "            ints.append(idx)\n",
        "        tokenized_reviews.append(ints)\n",
        "\n",
        "    return tokenized_reviews\n",
        "# We are trying to fetch the position of each word in embed_lookup(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GAlekiGiQXL",
        "outputId": "1430868a-06a2-4d80-bb22-89009fe38d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 137, 3, 0, 11620, 3799, 13, 1215, 10, 9, 194, 54, 12, 73, 61, 685, 41, 183, 243, 129, 12, 1663, 119, 72, 0, 9, 2989, 7334, 242, 159, 0, 453, 2, 0, 137, 1239, 19951, 3, 141, 1980, 0, 1898, 55, 3, 1663, 9, 11124, 0, 3857, 6663, 9, 20401, 295, 28, 45, 148, 157, 102, 27, 15452, 1663, 30714, 9, 65172, 0, 9, 844, 737, 47, 6585, 159, 0, 9, 668, 4365, 1003, 0, 27, 295, 56, 4365, 622, 9, 3832, 0, 43, 0, 897, 3187, 907, 0, 5396, 113, 9, 183, 4365, 1009, 3165, 10, 137, 0, 3288, 296, 10314, 4365, 6638, 213, 0, 8810, 40, 0, 116, 1663, 897, 2059, 0, 0, 137, 4365, 830, 2, 124, 2216, 0, 119, 782, 144, 2, 0, 137, 3, 330, 23046, 78, 0, 16915, 2, 13, 85275, 7451]\n"
          ]
        }
      ],
      "source": [
        "tokenized_reviews = tokenize_all_reviews(embed_lookup, reviews_split)\n",
        "print(tokenized_reviews[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kA2_rBH1klTd"
      },
      "outputs": [],
      "source": [
        "def pad_features(tokenized_reviews, seq_length):\n",
        "\n",
        "\n",
        "    features = np.zeros((len(tokenized_reviews), seq_length), dtype=int)\n",
        "\n",
        "    for i, row in enumerate(tokenized_reviews):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "\n",
        "    return features\n",
        "\n",
        "    #padding for convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT4UMbEVkpLG",
        "outputId": "3544990a-19da-4c8a-c4d3-bcf8bfb8983c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [ 16483     26      0     12 106210      0   1698     22     37     24\n",
            "     432      1     72     30    275      0    303      0    162    126]\n",
            " [  1935   1326     12      0   1403     60   3921   2019      3   4809\n",
            "      36      6   3172   7184    129   7951      0   2180   6098 166268]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0]\n",
            " [    56   4365      8    270    119    756    247    159    381      0\n",
            "       9   2669      0    148  21621     13      8     40      0    124]]\n"
          ]
        }
      ],
      "source": [
        "seq_length = 200\n",
        "\n",
        "features = pad_features(tokenized_reviews, seq_length=seq_length)\n",
        "\n",
        "assert len(features)==len(tokenized_reviews), \"Features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "print(features[:10,:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxiMbEF7kqGT",
        "outputId": "488c0f82-17e0-4723-b0de-f88200465a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(20000, 200) \n",
            "Validation set: \t(2500, 200) \n",
            "Test set: \t\t(2500, 200)\n"
          ]
        }
      ],
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "\n",
        "split_idx = int(len(features)*split_frac)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3sVjiupWktwJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "#pytorch understands numy arrays\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "#shuffle is to feed random data to model\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWqKzpzekwMl",
        "outputId": "6e4fdc1e-56f4-4f3f-dbca-c901ae57df90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, training on CPU.\n"
          ]
        }
      ],
      "source": [
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeNpmsv1k0nm"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SentimentCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The embedding layer + CNN model that will be used to perform sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_model, vocab_size, output_size, embedding_dim,\n",
        "                 num_filters=100, kernel_sizes=[3, 4, 5], freeze_embeddings=True, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentCNN, self).__init__()\n",
        "\n",
        "        self.num_filters = num_filters\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.embedding.weight = nn.Parameter(torch.from_numpy(embed_model.vectors)) # all vectors\n",
        "\n",
        "        if freeze_embeddings:\n",
        "            self.embedding.requires_grad = False\n",
        "            #we will not learn word embeddings\n",
        "\n",
        "        self.convs_1d = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_filters, (k, embedding_dim), padding=(k-2,0))\n",
        "            for k in kernel_sizes])\n",
        "\n",
        "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def conv_and_pool(self, x, conv):\n",
        "\n",
        "        x = F.relu(conv(x)).squeeze(3)\n",
        "\n",
        "\n",
        "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x_max\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines how a batch of inputs, x, passes through the model layers.\n",
        "        Returns a single, sigmoid-activated class score as output.\n",
        "        \"\"\"\n",
        "        embeds = self.embedding(x) # (batch_size, seq_length, embedding_dim)\n",
        "        embeds = embeds.unsqueeze(1)\n",
        "\n",
        "        conv_results = [self.conv_and_pool(embeds, conv) for conv in self.convs_1d]\n",
        "\n",
        "        x = torch.cat(conv_results, 1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        logit = self.fc(x)\n",
        "\n",
        "        return self.sig(logit)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWJl-D_tk28u",
        "outputId": "00b4b197-acef-432e-a30e-d31ac2c7f65d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentCNN(\n",
            "  (embedding): Embedding(299567, 300)\n",
            "  (convs_1d): ModuleList(\n",
            "    (0): Conv2d(1, 50, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
            "    (1): Conv2d(1, 50, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n",
            "    (2): Conv2d(1, 50, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n",
            "  )\n",
            "  (fc): Linear(in_features=150, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(pretrained_words)\n",
        "output_size = 1 # binary class (1 or 0)\n",
        "embedding_dim = len(embed_lookup[pretrained_words[0]]) # 300-dim vectors\n",
        "num_filters = 50\n",
        "kernel_sizes = [3, 4, 5]\n",
        "\n",
        "net = SentimentCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n",
        "                   num_filters, kernel_sizes)\n",
        "\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xt8FP4uIk5CH"
      },
      "outputs": [],
      "source": [
        "lr=0.0001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOdwHXxak6s_"
      },
      "outputs": [],
      "source": [
        "def train(net, train_loader, epochs, print_every=100):\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    net.train()\n",
        "    for e in range(epochs):\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            counter += 1\n",
        "\n",
        "            if(train_on_gpu):\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            net.zero_grad()\n",
        "\n",
        "            output = net(inputs)\n",
        "\n",
        "            loss = criterion(output.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if counter % print_every == 0:\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for inputs, labels in valid_loader:\n",
        "\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                    output = net(inputs)\n",
        "                    val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train()\n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "ZzCXDhe5k8yV",
        "outputId": "07a8efc4-a8a8-49a7-e798-003c9488c25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5... Step: 100... Loss: 0.689403... Val Loss: 0.686964\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-76f1a8eefc84>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprint_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-93596170bca6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_loader, epochs, print_every)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# training params\n",
        "\n",
        "epochs = 5\n",
        "print_every = 100\n",
        "\n",
        "train(net, train_loader, epochs, print_every=print_every)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFhTSqqnk-bS"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "\n",
        "\n",
        "net.eval()\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    output = net(inputs)\n",
        "\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb-yVvsAlCSA"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "\n",
        "def tokenize_review(embed_lookup, test_review):\n",
        "    test_review = test_review.lower() # lowercase\n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "\n",
        "    test_words = test_text.split()\n",
        "\n",
        "    tokenized_review = []\n",
        "    for word in test_words:\n",
        "        try:\n",
        "            idx = embed_lookup.vocab[word].index\n",
        "        except:\n",
        "            idx = 0\n",
        "        tokenized_review.append(idx)\n",
        "\n",
        "    return tokenized_review\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mwoc6l2UlGRa"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "\n",
        "def tokenize_review(embed_lookup, test_review):\n",
        "    test_review = test_review.lower() # lowercase\n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "\n",
        "    test_words = test_text.split()\n",
        "    print(test_words)\n",
        "    tokenized_review = []\n",
        "    for word in test_words:\n",
        "        try:\n",
        "            idx = embed_lookup.key_to_index[word]\n",
        "        except:\n",
        "            idx = 0\n",
        "        tokenized_review.append(idx)\n",
        "\n",
        "    return tokenized_review\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPDbplwnlVad"
      },
      "outputs": [],
      "source": [
        "def predict(embed_lookup, net, test_review, sequence_length=200):\n",
        "    \"\"\"\n",
        "    Predict whether a given test_review has negative or positive sentiment.\n",
        "    \"\"\"\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    test_ints = tokenize_review(embed_lookup, test_review)\n",
        "    print(test_ints)\n",
        "    seq_length=sequence_length\n",
        "    features = pad_features([test_ints], seq_length)\n",
        "\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "\n",
        "    batch_size = feature_tensor.size(0)\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        feature_tensor = feature_tensor.cuda()\n",
        "\n",
        "    output = net(feature_tensor)\n",
        "\n",
        "    pred = torch.round(output.squeeze())\n",
        "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
        "\n",
        "    if(pred.item()==1):\n",
        "        print(\"Positive review detected!\")\n",
        "    else:\n",
        "        print(\"Negative review detected.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ragc9FyIlbxZ"
      },
      "outputs": [],
      "source": [
        "seq_length=200\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hhn6p6buldCH"
      },
      "outputs": [],
      "source": [
        "# negative test review\n",
        "test_review_pos = 'Best Movie i have ever seen.'\n",
        "\n",
        "# test negative review\n",
        "predict(embed_lookup, net, test_review_pos, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf925UiFli5J"
      },
      "outputs": [],
      "source": [
        "# negative test review\n",
        "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n",
        "\n",
        "# test negative review\n",
        "predict(embed_lookup, net, test_review_neg, seq_length)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}