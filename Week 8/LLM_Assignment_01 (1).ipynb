{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#LLM Assignment 01#  \n",
        "##Part 1##  "
      ],
      "metadata": {
        "id": "x3EBmbDAFGTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.\tExplain Context window of LLMs in a few lines and why this metric is important ###  \n",
        "\n",
        "#### The context window of a Large Language Model (LLM) refers to the maximum amount of tokens the model can consider at once while generating a response. This metric is crucial because it determines how much information the model can use to understand and respond to a given input, affecting its ability to maintain coherence and relevance over longer passages. A larger context window allows the model to handle more extensive conversations, documents, and complex tasks more effectively. ####"
      ],
      "metadata": {
        "id": "NjSAICakFWJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.\tExplain the self attention mechanism in LLMs ###  \n",
        "####  The self-attention mechanism in Large Language Models (LLMs) allows the model to weigh the importance of different words in a sentence relative to each other. Here's a step-by-step explanation: ####\n",
        "\n",
        "1. **Input Representation**: Each word in the input sequence is first transformed into an embedding, which is a dense vector representation.\n",
        "\n",
        "2. **Query, Key, and Value Vectors**: For each word embedding, three vectors are computed: the Query (Q), Key (K), and Value (V) vectors. These vectors are learned transformations of the input embeddings.\n",
        "\n",
        "3. **Attention Scores**: The attention score between two words is calculated by taking the dot product of the Query vector of one word with the Key vectors of all words in the sequence. This results in a score that indicates the relevance of one word to another.\n",
        "\n",
        "4. **Softmax Function**: The scores are passed through a softmax function to obtain attention weights. These weights sum to 1 and represent the relative importance of each word in the context of the current word.\n",
        "\n",
        "5. **Weighted Sum**: Each word's Value vectors are then weighted by the attention weights and summed to produce a new representation for each word. This representation captures the contextual information from the entire sequence, highlighting the most relevant words.\n",
        "\n",
        "6. **Output**: The resulting weighted sums are combined to form the final output of the self-attention mechanism.\n",
        "\n",
        "#### Self-attention enables the model to consider the entire input sequence simultaneously, capturing dependencies between words regardless of their positions. This is crucial for tasks that require understanding context, such as translation, summarization, and language modeling.####"
      ],
      "metadata": {
        "id": "6WG7BS3NGGx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.\tConsider you have an enterprise data  you need any LLM to answer from, what are all techniques available to achieve this? ####  \n",
        "\n",
        "#### We have two techniques available:  \n",
        "a. Retrieval-Augmented Generation (RAG)  \n",
        "RAG combines a retrieval mechanism with a generative model. It first fetches relevant documents from a database using a retrieval model (like BM25 or dense retrievers) and then uses an LLM to generate responses based on this information. This ensures accurate, contextually relevant answers from large datasets.\n",
        "\n",
        "b. Fine-Tuning  \n",
        "Fine-tuning involves training an LLM on specific enterprise data to specialize it for particular tasks or domains. By updating the model's parameters with domain-specific data, it becomes more adept at generating accurate and relevant responses based on the enterprise's unique information. This approach requires substantial data and computational resources."
      ],
      "metadata": {
        "id": "9hkhFqjNmgCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.\tExplain what is meant by Quantization of a LLM ###  \n",
        "####  Quantization of a Large Language Model (LLM) refers to the process of reducing the precision of the model's weights and activations from floating-point (e.g., 32-bit or 16-bit) to lower precision (e.g., 8-bit or even lower). This process significantly reduces the model's memory footprint and computational requirements, enabling faster inference and lower energy consumption without substantial loss in performance. Quantization is often used to deploy LLMs on resource-constrained devices or to improve efficiency in large-scale applications. ####"
      ],
      "metadata": {
        "id": "R_s5S8srokLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.\tAfter an LLM has been pre trained, which procedure it goes through to Give Answer in a Q nd A format? ###  \n",
        "#### After an LLM has been pre-trained, it follows this procedure to give an answer in a Q&A format using Retrieval-Augmented Generation (RAG):\n",
        "\n",
        "1. **Query Input:** The user's question is input as a query.\n",
        "2. **Embedding Search:** The query is transformed into embeddings and searched in a vector database containing pre-computed document embeddings.\n",
        "3. **Context Retrieval:** Relevant documents (top-K) are retrieved based on their similarity to the query embeddings and provided as context.\n",
        "4. **Response Generation:** The query and retrieved context are combined into a prompt template, which is then processed by the LLM to generate a contextually accurate answer.\n",
        "\n",
        "This process ensures the model leverages both its pre-trained knowledge and specific information from the retrieved documents to provide precise answers."
      ],
      "metadata": {
        "id": "JPg3xbtIpKOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 ##"
      ],
      "metadata": {
        "id": "9tOxvr1erh8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STT Whisper model to convert an audio file to Text: (Use Hugging face Transformers Library) ###"
      ],
      "metadata": {
        "id": "BOlpMY4rrrab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your audio file\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/sample1.flac'"
      ],
      "metadata": {
        "id": "xefmMTpUsaof",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa824efa-b454-4fbe-f0fc-34d4afa249f9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "# Function to read audio file\n",
        "def read_audio(file_path):\n",
        "    waveform, sample_rate = sf.read(file_path)\n",
        "    # Ensure the waveform is a 1D numpy array\n",
        "    if waveform.ndim > 1:\n",
        "        waveform = waveform.mean(axis=1)  # Convert to mono if stereo\n",
        "    return waveform, sample_rate"
      ],
      "metadata": {
        "id": "i012nNCSmu9A"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "# load model and processor\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "model.config.forced_decoder_ids = None\n",
        "\n",
        "# load dummy dataset and read audio files\n",
        "waveform, sample_rate = read_audio(file_path)\n",
        "input_features = processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\").input_features\n",
        "\n",
        "# generate token ids\n",
        "predicted_ids = model.generate(input_features)\n",
        "\n",
        "# decode token ids to text\n",
        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEE9zTrDEE2j",
        "outputId": "7b3c6ba2-fc8d-4629-bffa-72038f510817"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pArTQOaIr9PR",
        "outputId": "5ee52a36-790d-4ef2-c8ce-c0f15a660f3f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\" going along slushy country roads and speaking to damp audiences in drafty school rooms day after day for a fortnight. He'll have to put in an appearance at some place of worship on Sunday morning, and he can come to us immediately afterwards.\"]\n"
          ]
        }
      ]
    }
  ]
}