{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALk1Yst6UpI3"
      },
      "source": [
        "Import the main PyTorch package along with the *nn* package which is used to build the model. numpy will be used to pre-process data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pgFHSK-KUpI3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7BNWTfSUpI5"
      },
      "source": [
        "Define a custom dataset of sentences that model should output when fed with the first word or the first few characters.\n",
        "\n",
        "This is followed by creating a dictionary out of all the characters in the sentences and map them to an integer. This will allow to convert  input characters to their respective integers (*char2int*) and vice versa (*int2char*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vsUmESI1UpI5"
      },
      "outputs": [],
      "source": [
        "text = ['hey how are you','good i am fine','have a nice day']\n",
        "\n",
        "# Join all the sentences together and extract the unique characters from the combined sentences\n",
        "chars = set(''.join(text))\n",
        "\n",
        "# Creating a dictionary that maps integers to the characters\n",
        "int2char = dict(enumerate(chars))\n",
        "\n",
        "# Creating another dictionary that maps characters to integers\n",
        "char2int = {char: ind for ind, char in int2char.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2PpTG5svUpI6",
        "outputId": "d55dd6b9-fbab-45c8-aa12-dafee2802413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'h': 0, 'm': 1, ' ': 2, 'u': 3, 'e': 4, 'a': 5, 'y': 6, 'r': 7, 'g': 8, 'w': 9, 'd': 10, 'f': 11, 'v': 12, 'c': 13, 'n': 14, 'i': 15, 'o': 16} dict_items([(0, 'h'), (1, 'm'), (2, ' '), (3, 'u'), (4, 'e'), (5, 'a'), (6, 'y'), (7, 'r'), (8, 'g'), (9, 'w'), (10, 'd'), (11, 'f'), (12, 'v'), (13, 'c'), (14, 'n'), (15, 'i'), (16, 'o')])\n"
          ]
        }
      ],
      "source": [
        "print(char2int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KSKbNQYUpI7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(text, key=len) # Gives the longest sentence in the list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yO3i_caIdPai",
        "outputId": "0041d591-1440-4664-cc07-c9a5ab27ac08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hey how are you'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MDTf3xHAUpI7",
        "outputId": "1d2e2e76-08fa-45be-ed0d-9fb23e9e3b51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The longest string has 15 characters\n"
          ]
        }
      ],
      "source": [
        "# Padding\n",
        "\n",
        "# Adds a ' ' whitespace until the length of the sentence matches the length of the longest sentence\n",
        "\n",
        "maxlen = len(max(text, key=len))\n",
        "print(\"The longest string has {} characters\".format(maxlen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eDGCwqTfUpI7"
      },
      "outputs": [],
      "source": [
        "for i in range(len(text)):\n",
        "    while len(text[i])<maxlen:\n",
        "        text[i] += ' ' # padding spaces at the end of each sentence if required"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XoIjA-3UpI7"
      },
      "source": [
        "To predict the next character in the sequence at each time step, divide each sentence into\n",
        "\n",
        "- Input data\n",
        "    - The last input character should be excluded as it does not need to be fed into the model\n",
        "- Target/Ground Truth Label\n",
        "    - One time-step ahead of the Input data as this will be the \"correct answer\" for the model at each time step corresponding to the input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QWKewIyNUpI8",
        "outputId": "6d649e03-1cb4-4c4c-91c3-65486d9de4e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: hey how are yo\n",
            "Target Sequence: ey how are you\n",
            "Input Sequence: good i am fine\n",
            "Target Sequence: ood i am fine \n",
            "Input Sequence: have a nice da\n",
            "Target Sequence: ave a nice day\n"
          ]
        }
      ],
      "source": [
        "# Creating lists that will hold our input and target sequences\n",
        "input_seq = []\n",
        "target_seq = []\n",
        "\n",
        "for i in range(len(text)):\n",
        "    # Remove last character for input sequence\n",
        "    input_seq.append(text[i][:-1])\n",
        "\n",
        "    # Remove firsts character for target sequence\n",
        "    target_seq.append(text[i][1:])\n",
        "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5Qp4YT2UpI8"
      },
      "source": [
        "Convert input and target sequences to sequences of integers instead of characters by mapping them using the dictionaries we created above.\n",
        "\n",
        "This will allow to one-hot-encode input sequence subsequently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C-16twXAUpI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70bd077e-4116-4bbc-96ec-09d32985d4cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: [0, 5, 12, 4, 2, 5, 2, 14, 15, 13, 4, 2, 10, 5]\n",
            "Target Sequence: [5, 12, 4, 2, 5, 2, 14, 15, 13, 4, 2, 10, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(text)):\n",
        "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
        "    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",
        "\n",
        "print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0hyK74qUpI8"
      },
      "source": [
        "Before encoding input sequence into one-hot vectors, define 3 key variables:\n",
        "\n",
        "- *dict_size*: The number of unique characters that we have in our text\n",
        "    - This will determine the one-hot vector size as each character will have an assigned index in that vector\n",
        "- *seq_len*: The length of the sequences that we're feeding into the model\n",
        "    - As we standardised the length of all our sentences to be equal to the longest sentences, this value will be the max length - 1 as we removed the last character input as well\n",
        "- *batch_size*: The number of sentences that we defined and are going to feed into the model as a batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gKL3mGUSUpI8"
      },
      "outputs": [],
      "source": [
        "dict_size = len(char2int)\n",
        "seq_len = maxlen - 1\n",
        "batch_size = len(text)\n",
        "\n",
        "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
        "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
        "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "\n",
        "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
        "    for i in range(batch_size):\n",
        "        for u in range(seq_len):\n",
        "            features[i, u, sequence[i][u]] = 1\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSCl0_JJUpI9"
      },
      "source": [
        "Defined a helper function that creates arrays of zeros for each character and replaces the corresponding character index with a **1**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bV26P0opUpI9",
        "outputId": "03a25d21-5175-4723-b85a-7df69a725168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (3, 14, 17) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n"
          ]
        }
      ],
      "source": [
        "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
        "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7HX2qOdUpI9"
      },
      "source": [
        "After data pre-processing, move the data from numpy arrays to PyTorch's own data structure - **Torch Tensors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HA2EmRX8UpI9"
      },
      "outputs": [],
      "source": [
        "input_seq = torch.from_numpy(input_seq)\n",
        "target_seq = torch.Tensor(target_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bsyLGiUUpI9"
      },
      "source": [
        "Now define the model using the Torch library.\n",
        "Add or remove layers like fully connected , convolutational layers, vanilla RNN layers, LSTM layers, and many more!\n",
        "\n",
        "In this post, we'll use the basic nn.rnn to demonstrate a simple example of how RNNs can be used.\n",
        "\n",
        "First check if the device is running on (CPU or GPU).\n",
        "This implementation will not require GPU as the training is really simple. However, for large datasets and models with millions of trainable parameters, using the GPU will be very important to speed up training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Zx0pBOPeUpI9",
        "outputId": "5c8378e1-f331-4dc3-a64f-24ea9d158318",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU not available, CPU used\n"
          ]
        }
      ],
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KcLPhKJUpI-"
      },
      "source": [
        "To start the neural network model, define a class that inherits PyTorch’s base class (nn.module) for all neural network modules.\n",
        "\n",
        "After that, start defining some variables and also the layers for model under the constructor.\n",
        "\n",
        "For this model, we will use 1 layer of RNN followed by a fully connected layer. The fully connected layer will be in-charge of converting the RNN output to our desired output shape.\n",
        "\n",
        "Than define the forward pass function under forward() as a class method. The order the forward function is sequentially executed, therefore we first pass the inputs and the zero-initialized hidden state through the RNN layer first, before passing the RNN outputs to the fully-connected layer. Note that we are using the layers that we defined in the constructor.\n",
        "\n",
        "The last method that we have to define is the method that we called earlier to initialize the hidden state - init_hidden(). This basically creates a tensor of zeros in the shape of our hidden states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iIu6wrjCUpI-"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        #Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "\n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDEBsjHlUpI-"
      },
      "source": [
        "After defining the model above, we'll have to instantiate the model with the relevant parameters and define our hyperparamters as well. The hyperparameters we're defining below are:\n",
        "\n",
        "- *n_epochs*: Number of Epochs --> This refers to the number of times our model will go through the entire training dataset\n",
        "- *lr*: Learning Rate --> This affects the rate at which our model updates the weights in the cells each time backpropogation is done\n",
        "    - A smaller learning rate means that the model changes the values of the weight with a smaller magnitude\n",
        "    - A larger learning rate means that the weights are updated to a larger extent for each time step\n",
        "\n",
        "Similar to other neural networks, we have to define the optimizer and loss function as well. We’ll be using CrossEntropyLoss as the final output is basically a classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rkbmLZJHUpI_"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model with hyperparameters\n",
        "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 100\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcwwoPiQUpI_"
      },
      "source": [
        "Now we can begin our training! As we only have a few sentences, this training process is very fast. However, as we progress, larger datasets and deeper models mean that the input data is much larger and the number of parameters within the model that we have to compute is much more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "scrolled": true,
        "id": "DLS_hfYAUpI_",
        "outputId": "43d4d2c7-d4d4-45e4-ffb1-2d436d24c351",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/100............. Loss: 0.1348\n",
            "Epoch: 20/100............. Loss: 0.1113\n",
            "Epoch: 30/100............. Loss: 0.0959\n",
            "Epoch: 40/100............. Loss: 0.0852\n",
            "Epoch: 50/100............. Loss: 0.0775\n",
            "Epoch: 60/100............. Loss: 0.0716\n",
            "Epoch: 70/100............. Loss: 0.0669\n",
            "Epoch: 80/100............. Loss: 0.0632\n",
            "Epoch: 90/100............. Loss: 0.0602\n",
            "Epoch: 100/100............. Loss: 0.0576\n"
          ]
        }
      ],
      "source": [
        "# Training Run\n",
        "input_seq = input_seq.to(device)  # Move the input sequence to the specified device (CPU or GPU)\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):  # Loop over the number of epochs\n",
        "    optimizer.zero_grad()  # Clears existing gradients from the previous epoch to prevent accumulation\n",
        "\n",
        "    # Forward pass: compute the output and hidden state of the model\n",
        "    output, hidden = model(input_seq)\n",
        "\n",
        "    # Move the output to the specified device (CPU or GPU)\n",
        "    output = output.to(device)\n",
        "\n",
        "    # Move the target sequence to the specified device (CPU or GPU)\n",
        "    target_seq = target_seq.to(device)\n",
        "\n",
        "    # Compute the loss between the model output and the target sequence\n",
        "    # The target sequence is reshaped to match the dimensions expected by the loss function\n",
        "    loss = criterion(output, target_seq.view(-1).long())\n",
        "\n",
        "    loss.backward()  # Perform backpropagation to compute gradients of the loss with respect to model parameters\n",
        "\n",
        "    optimizer.step()  # Update the model parameters using the computed gradients\n",
        "\n",
        "    if epoch % 10 == 0:  # Every 10 epochs, print the current epoch number and loss\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))  # Print the loss formatted to 4 decimal places"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhzaRwJvUpI_"
      },
      "source": [
        "Let’s test our model now and see what kind of output we will get. Before that, let’s define some helper function to convert our model output back to text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gqu3YbPRUpI_"
      },
      "outputs": [],
      "source": [
        "def predict(model, character):\n",
        "    # One-hot encoding our input to fit into the model\n",
        "    # Convert the input character sequence to its corresponding integer representation using char2int dictionary\n",
        "    character = np.array([[char2int[c] for c in character]])\n",
        "\n",
        "    # One-hot encode the integer representation to create an input tensor suitable for the model\n",
        "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
        "\n",
        "    # Convert the numpy array to a PyTorch tensor\n",
        "    character = torch.from_numpy(character)\n",
        "\n",
        "    # Move the tensor to the specified device (CPU or GPU)\n",
        "    character = character.to(device)\n",
        "\n",
        "    # Pass the input tensor through the model to get the output and the hidden state\n",
        "    out, hidden = model(character)\n",
        "\n",
        "    # Apply the softmax function to the output to get the probabilities of each character\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "\n",
        "    # Find the index of the character with the highest probability\n",
        "    char_ind = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "    # Return the character corresponding to the highest probability index and the hidden state\n",
        "    return int2char[char_ind], hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5vwdbVCiUpI_"
      },
      "outputs": [],
      "source": [
        "def sample(model, out_len, start='hey'):\n",
        "    # Set the model to evaluation mode (disables dropout, batch norm, etc.)\n",
        "    model.eval()\n",
        "\n",
        "    # Convert the starting string to lowercase\n",
        "    start = start.lower()\n",
        "\n",
        "    # Create a list of characters from the starting string\n",
        "    chars = [ch for ch in start]\n",
        "\n",
        "    # Calculate how many more characters we need to generate\n",
        "    size = out_len - len(chars)\n",
        "\n",
        "    # Generate characters until we reach the desired output length\n",
        "    for ii in range(size):\n",
        "        # Use the predict function to get the next character and the hidden state\n",
        "        char, h = predict(model, chars)\n",
        "\n",
        "        # Append the predicted character to the list\n",
        "        chars.append(char)\n",
        "\n",
        "    # Join the list of characters into a single string and return it\n",
        "    return ''.join(chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AVEekPahUpJA",
        "outputId": "1e81824c-e6cd-49ee-c118-d1ba04371aca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hey how are youe are'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "sample(model, 20, 'hey')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RSJmQxdUpJA"
      },
      "source": [
        "As we can see, the model is able to come up with the sentence ‘good i am fine ‘ if we feed it with the words ‘good’, achieving what we intended for it to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "735zQOlPUpJA"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'checkpoint.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load checkpoint from checkpoint.pth into model1 variable\n",
        "\n",
        "model1 = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
        "model1.load_state_dict(torch.load('checkpoint.pth'))\n",
        "model1.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aWWaAzICZMt",
        "outputId": "87f73ddd-ce59-4e31-95e3-72ee8af84a86"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (rnn): RNN(17, 12, batch_first=True)\n",
              "  (fc): Linear(in_features=12, out_features=17, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample1(model, out_len, start='hey'):\n",
        "    model1.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    # First off, run through the starting characters\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    # Now pass in the previous characters and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model1, chars)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "7nQjwVExCkp3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample1(model1, 15, 'good')"
      ],
      "metadata": {
        "id": "OWe8FFdYCoCx",
        "outputId": "497e3e0d-195a-4826-8f36-461cbd5b3ec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good i am fine '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TensorFlow-GPU",
      "language": "python",
      "name": "tf_gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}